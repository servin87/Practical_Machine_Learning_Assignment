---
title: "Prediction Assignment Writeup"
author: "Jesus Felix"
date: "August/2020"
output: html_document
---
<style>
body {
text-align: justify}
</style>


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```
## Introduction
In this report, data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants is used to predict the manner in which barbell lifts were performed. Each participant was asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available [here](http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har). The goal is to predict how the exercise was performed (`classe`) based on the sensors readings. We have been provided with a train and a test dataset. While the training dataset contains the `classe` for each observation, the testing does not. The objective is to create and train a prediction model to determine the `classe` for each observation of the test set. 

## Exploratory Data Analysis
We begin by loading the datasets (train and test) and conducting an exploratory analysis on the training set. The data consists of 160 variables, and 19622 observations for the training set and 20 for the test set. While loading the data, I replaced all cells that contain `NA`, `Nan`, `" "` and `""` to `NA` to make it easier to locate and quantify missing data.  
```{r message=FALSE, warning=FALSE}
library(sjPlot)
train <- read.csv('pml-training.csv', header=T, na.strings=c("NA","NaN", " ", ""))
test <- read.csv('pml-testing.csv', header=T, na.strings=c("NA","NaN", " ",""))
dim_data<-data.frame(Rows=c(dim(train)[1],dim(test)[1]), Columns=c(dim(train)[2],dim(test)[2]), row.names = c("Train", "Test"))
dim_data
#tab_df(dim_data,show.rownames = TRUE)
```
Looking at the variables, I noticed that the first seven are not related to the sensors measurements and thus irrelevant to this study, because we are trying to predict how the exercise was performed based on the data coming from the sensors. Therefore, I removed the first seven variables from both sets.
```{r}
names(train)[1:7]
train<-train[,-(1:7)]
test<-test[,-(1:7)]
p_id<- test$problem_id
#dim(train)
```

Next, I checked for columns with `NA` and removed all variables with a percentage of `NA` higher than 50%. In fact, it appears that the columns either have no `NA` or ~98% `NA`. After removing variables with a high proportion of missing data, 53 variables remain, including the `classe` which is what we want to predict. 
```{r}
nas<-as.data.frame(sapply(train, function(x) sum(is.na(x))/dim(train)[1]))
names(nas)<-c("Percentage")
col_remove<-nas$Percentage<0.5
col_names<-names(train)
train<-train[,col_names[col_remove]]
test<-test[,col_names[col_remove][1:length(col_names[col_remove])-1]]
dim(train)
```
Before proceeding to build the prediction model, I used `nearZeroVar()` to evaluate if any of the remaining variables had little variance and was therefore, candidate to be excluded. The results indicate that none of the considered predictors meet the near zero variance criteria. I therefore decided to use all 52 variables to train the prediction models. I have also checked the frequency of each `classe` (**Fig. 1**). While `A` seems to be over represented, I decided that the difference is not big enough to justify down-sampling the data. It is worth mentioning that trying to balance the data brings its own uncertainty. In addition, it is possible that `A` has higher frequency in a real scenario because sports enthusiasts are likely to perform their exercises correctly. 
```{r message=FALSE, warning=FALSE, fig.align="center", fig.cap="**Fig. 1 Frequency of each classe**"}
library (caret)
nearZeroVar(train)
barplot(table(train$classe), ylab = "Frequency", xlab = "Classe")
```

As a final step before proceeding to build the model, I decided to split the training set into a training (70%) and validation (30%) sets, to get a better estimate of how my model will perform in new data and of the out of sample error.
```{r}
set.seed(9753)
splitTrain <- createDataPartition(train$classe, p = 0.7, list = FALSE)
train_set <- train[splitTrain,]
validation_set <- train[-splitTrain,]
dim_data<-data.frame(Rows=c(dim(train_set)[1],dim(validation_set)[1]), Columns=c(dim(train_set)[2],dim(validation_set)[2]), row.names = c("Train", "Validation"))
dim_data
```
## Model selection
There are multiple approaches suitable for this kind of classification. I have decided to compare three of them and choose the best performing one for the prediction exercise. For the purpose of this analysis I have chosen a Random Forest (RF) classifier, Gradient Boosting Machine (GBM) and Recursive Partitioning And Regression Trees (RPART). 10-fold cross validation was performed for all models. 
```{r fig.align="center", message=FALSE, warning=FALSE}
library(knitr)
if (!file.exists("mod_rf.rda")) {
  train_ctrl <- trainControl(method = "cv", number = 10) 
  mod_rf <- train(classe ~ .,
                  data = train_set,
                  method = "rf",
                  trControl = train_ctrl,
                  do.trace = 500,
                  verbose = FALSE, importance=TRUE)
  save(mod_rf, file = "mod_rf.rda")
} else {
  load("mod_rf.rda")
}

if (!file.exists("mod_gbm.rda")) {
  train_ctrl <- trainControl(method = "cv", number = 10) 
  mod_gbm <- train(classe ~ ., 
                    data=train_set,
                    trControl=train_ctrl,
                    method="gbm")
  save(mod_gbm, file = "mod_gbm.rda")
} else {
  load("mod_gbm.rda")
} 
  
if (!file.exists("mod_cart.rda")) {
  train_ctrl <- trainControl(method = "cv", number = 10) 
  mod_cart <- train(classe ~ ., 
                    data=train_set,
                    trControl=train_ctrl,
                    method='rpart')
  save(mod_cart, file = "mod_cart.rda")
} else {
  load("mod_cart.rda")
}  
  
pred_rf <- predict(mod_rf, newdata = validation_set)
pred_gbm <- predict(mod_gbm, newdata = validation_set)
pred_cart <- predict(mod_cart, newdata = validation_set)
cm_rf <- confusionMatrix(pred_rf, validation_set$classe)
cm_gbm <- confusionMatrix(pred_gbm, validation_set$classe)
cm_cart <- confusionMatrix(pred_cart, validation_set$classe)

tcm_rf<-tab_df(as.data.frame(as.matrix(cm_rf)), title="Confusion Matrix for RF", CSS=list(css.table = 'border:2px solid black;'))
ta_rf<-tab_df(cm_rf$overall, title="Accuracy for RF model")
tcm_gbm<-tab_df(as.data.frame(as.matrix(cm_gbm)), title="Confusion Matrix for GBM")
ta_gbm<-tab_df(cm_gbm$overall, title="Accuracy for GBM model")
tcm_cart<-tab_df(as.data.frame(as.matrix(cm_cart)), title="Confusion Matrix for CART", CSS=list(css.table = 'border:2px solid black;'))
ta_cart<-tab_df(cm_cart$overall, title="Accuracy for CART model")
```

The results show that the RF fit has the highest accuracy on the validation test (`r round(cm_rf$overall['Accuracy'],2)*100`%), followed closely by the GBM fit (`r round(cm_gbm$overall['Accuracy'],2)*100`%). The accuracy of the RPART fit is significantly lower, at `r round(cm_cart$overall['Accuracy'],2)*100`%. 
<center>
`r ta_rf$knitr`
`r ta_gbm$knitr`
`r ta_cart$knitr`
</center>

Looking at the confusion matrix for each model it can be seen that RF is the only one that predicts at least one `classe` with 100% accuracy. Interestingly, that `classe` is `A`, which has the highest accuracy on all models. This could be a consequence of `A` being over represented in the train set. It is worth mentioning that it is not surprising that RF results in such high accuracy. It is well known that RF typically achieves high accuracy. Based on the results, I have decided to use the RF model for the prediction exercise. I expect the out of sample error to be less than 1%, based on the validation exercise. This value may however be over optimistic and the actual out of sample error may be greater.
<center>
`r kable(list(tcm_rf$knitr, tcm_gbm$knitr, tcm_cart$knitr))`
</center>
By plotting the variable importance for the RF model we can see that yaw_belt and roll_belt are the predictors contributing the most. We can also use this plot to remove some of the predictors that contribute little, which would reduce the processing time. However, given that our data set is not too big I have decided to stick with the 52 predictors previously chosen. The RF model has been saved and will be used for the prediction exercise of the assignment. The results of the predictions are not shown here but have been entered to the Course platform and resulted in 100% match. 
```{r importance-plot, fig.star=TRUE, fig.width=14, message=FALSE, warning=FALSE}
library(randomForest)
varImpPlot(mod_rf$finalModel, n.var=20, main="")
``` 

## Conclusion
This report presents the whole process of data science, from loading the data, performing exploratory analysis, cleaning the data, training a prediction model and validating the model. Three different algorithms were tested, including RF, GBM and CART. RF is well known for achieving high accuracy, even when compared with more complex approaches. In this example, RF indeed achieved the highest accuracy in the validation set among the three tested approached. The final model was used to predict the `classe` of the testing set and the predictions matched the right answer. 